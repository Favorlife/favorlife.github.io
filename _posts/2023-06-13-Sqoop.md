---
layout: post
read_time: true
show_date: true
title: "Sqoop 资料"
date: 2022-10-14
img: posts/20230613/1.png
tags: [Sqoop, BigData]
category: opinion
author: White Cool
description: "Sqoop复习资料"
---

# Sqoop复习

## Chapter 1

### 概述

1. Sqoop是一个在Hadoop分布式文件系统(HDFS)和关系数据库管理系统(RDBMS)之间高效传输批量数据的工具。

2. Sqoop可以将关系数据库中的数据导入Hadoop HDFS，包括Hive、HBase和其他基于HDFS的数据存储
3. Sqoop可以将数据从HDFS依次导出到关系数据库MySQL、Oracle、PostgreSQL等



### **Sqoop1与sqoop2**

|      | sqoop1                                                       | sqoop2                                                       |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 优点 | 基于连接器，架构简单，易于部署。可用于生产环境               | 引入了Sqoop Server对连接器或其他第三方插件进行集中管理，支持多种方式访问Sqoop Server: CLI、Web UI、REST API，引入了基于角色的安全机制，管理员可以在Sqoop Server上配置不同的角色 |
| 缺点 | 只能通过CLI(命令行)调用，使用错误的参数往往会出错，类型映射定义不清晰，安全顾虑，sqoop客户端需要直接连接Hadoop和数据库e .连接器必须符合JDBC模型 | 稍微复杂一点的体系结构，难以配置和部署，不完整的特性，无法在生产环境中使用 |



### Sqoop基本功能与原理

| 类型     | Sqoop                                                        |
| -------- | ------------------------------------------------------------ |
| 工作原理 | Sqoop的工作原理是将导入或导出命令转换为MapReduce作业。在翻译后的MapReduce作业中，可以自定义InputFormat和OutputFormat来实现导入或导出功能，Sqoop可以利用MapReduce的并行性和容错性来实现导入和导出操作。 |
| 导入规则 | Sqoop-import tool将单个RDBMS表导入到HDFS上，表中的每一行都表示为HDFS中的单独记录，记录可以存储为文本文件或者以二进制表示形式存储为Avro或SequenceFile数据格式。还有一个sqoop-import-all-tables tool,用于一次将多个表从RDBMS导入到HDFS，每个表的数据存储在一个单独的文件夹中 |
| 导入工具 | 有三种不同的方式：Full import（全量导入）、Partial import（部分导入）和Incremental import（增量导入）。Full import导入全量数据，Partial import导入部分列或部分行数据，Incremental import 只导入新数据 |
| 导出规则 | sqoop-export tool用于将一组文件从HDFS导出回RDBMS，当前目标表必须已经存在于数据库中，sqoop-export tool根据用户指定的分隔符读取输入文件，并将其解析为一组记录 |
| 导出工具 | 有三种不同的方式：Insert mode（插入模式）、Update mode（更新模式）、Call mode（调用模式）。Insert mode默认操作是使用INSERT语句将文件中的数据插入到表中，Update mode则是Sqoop将使用UPDATE语句替换数据库中的现有记录。Call mode则是Sqoop将从数据库中调用一个存储过程来导出每条记录。 |

**Sqoop查看表和数据库：**

`sqoop-list-databases`查看数据库，jdbc-uri连接时无需指定数据库

`sqoop-list-tables`查看数据库中的表，jdbc-uri连接时需指定数据库



---

## Chapter2

**Sqoop工具，以下为sqoop简单导出语句的总括:**

```shell
# 通用参数
sqoop --connect <jdbc-uri> \
--connect-manager <class-name> \
--driver <class-name> \
--hadoop-mapred-home <dir> \
--help \
--password <password> \ 
--password-file <filepath> \ 
-P \
--username <username> \
--verbose \
--hadoop-home <dir> \
--connection-param-file <filename> \
--relaxed-isolation 
```

**以下仅为不常用的语句的简介：**

| 参数                    | 描述                                                         |
| ----------------------- | ------------------------------------------------------------ |
| --connect-manager       | 指定要使用的连接管理器类（了解即可）                         |
| --driver                | 指定数据库驱动类（了解即可）                                 |
| --hadoop-mapred-home    | 重写$HADOOP_MAPRED_HOME（了解即可）                          |
| -P                      | 读取控制台上的密码                                           |
| --password-file         | 文件要保存在具有400权限的目录下，目录既可以是本地文件目录也可以是在hdfs上 |
| --verbose               | 运行期间打印更多信息（了解即可）                             |
| --hadoop-home           | 重写$HADOOP_HOME（了解即可）                                 |
| --connection-param-file | 为连接参数提供可选的属性文件（了解即可）                     |
| --relaxed-isolation     | 设置连接事务隔离以读取未提交的映射器（了解即可）             |

```shell
# 支持的通用选项
sqoop -conf <configuration file> \
-D <property=value> \
-fs <local|namenode:port> \
-jt <local|jobtracker:port> \
-files <comma separated list of files> \
-libjars <comma separated list of files> \
-archives <comma separated list of archives>
```

**解释：**

| 参数      | 描述                                                   |
| --------- | ------------------------------------------------------ |
| -conf     | 指定应用程序配置文件                                   |
| -D        | 使用给定属性的值                                       |
| -fs       | 指定名称结点（了解即可）                               |
| -jt       | 指定工作跟踪器（了解即可）                             |
| -files    | 指定要复制到 map reduce 集群的逗号分隔文件（了解即可） |
| -libjars  | 指定以逗号分隔的 jar 文件以包含在类路径中（了解即可）  |
| -archives | 指定要在计算机上取消存档的逗号分隔存档（了解即可）     |

>注意：
>
>通用命令行语法是：bin/hadoop command [genericOptions] [commandOptions]
>
>-conf、-D、-fs和-jt参数控制配置和Hadoop服务器设置









---

## Chapter 3

### Sqoop导入工具的使用

```shell
sqoop import (generic-args) (import-args)
```

**以下为导入所有指令：**

```shell
sqoop import --connect <jdbc-uri> --username <username> --password <password> \
--append \  # 将数据追加到HDFS的现有数据集
--as-avrodatafile \ # 将数据导入Avro数据文件中
--as-sequencefile \ # 将数据导入Sequence文件中
--as-textfile \ # 以纯文本格式导入数据（默认）
--as-parquetfile \ # 将数据导入到Parquet文件中
--boundary-query <statement> \ #用于创建分割的边界查询
--columns <col, col, col, ...> \ # 要从表中导入的列 
--delete-target-dir \ # 如果存在导入目标目录则删除该目录
--direct \ # 如果数据库存在直接连接器，则使用直接连接器
--fetch-size <n> \ # 一次从数据库中读取的条目数
--inline-lob-limit <n> \ # 设置内联LOB的最大大小
-m, --num-mappers <n> \ # 用n个map task进行并行导入
-e, --query <statement> \ # 导入语句的查询结果
--split-by <column-name> \ # 用于拆分工作单位的表列，不能与--autoreset-to-one-mapper共用
--split-limit <n> \ # 每个拆分大小的上限。这只适用于整数和日期列。对于日期或时间戳字段，以秒为单位计算。
--autoreset-to-one-mapper \ # 如果表没有主键并且没有提供拆分列，则导入应该使用的一个映射器。不能与--split-by共有
--table <table-name> \ # 指定表
--target-dir <dir> \ # HDFS的目标目录
--temporary-rootdir <dir> \ # 导入过程中创建的临时文件的HDFS目录（覆盖默认的"sqoop"）
--warehouse-dir <dir> \ # 表目标的HDFS父级
--where <where clause> \ # 导入期间要使用的WHERE子句
-z, --compress \ # 启用压缩
--compression-codec <c> \ # 使用Hadoop编解码器(默认gzip)
--null-string <null-string> \ # 要为字符串列的空值写入的字符串
--null-non-string <null-string> \ # 要为非字符串列的空值写入的字符串
--enclosed-by <char> \ # 设置包含字符的必填字段
--escaped-by <char> \ # 设置转义字符
--fields-terminated-by <char> \ # 设置字段分隔符
--lines-terminated-by <char> \ # 设置行尾字符
--mysql-delimiters \ # 使用mysql的默认分隔符集：fields:,lines:\n escaped-by: \ optionally-enclosed-by:'
--optionally-enclosed-by <char> \ # 设置包含字符的字段
--input-enclosed-by <char> \ # 设置封闭的必填字段
--input-escaped-by <char> \ # 设置输入的转义字符
--input-fields-terminated-by <char> \ # 设置输入字段分隔符
--input-lines-terminated-by <char> \ # 设置输入行尾字符
--input-optionally-enclosed-by <char> \ # 设置包含字符的字段
--hive-home <dir> \ # 覆盖$HIVE_HOME
--hive-import \ # 将表导入配置单元（如果没有设置分隔符，则使用配置单元的默认分隔符）
--hive-overwrite \ # 覆盖hive表存在的数据
--create-hive-table \ # 如果设置，那么如果目标配置单元表存在，作业将失败.该属性默认值为false
--hive-table <table-name> \ # 设置导入到配置单元时要使用的表名
--hive-drop-import-delims \ # 导入到配置单元时，从字符串字段中删除\n、\r和\01
--hive-delims-replacement \ # 导入到配置单元时，将字符串字段中的\n、\r和\01替换为用户定义的字符串
--hive-partition-key \ # 共享分区的配置单元字段的名称
--hive-partition-value <v> \ # String-value在此作业中用作导入到配置单元中的分区键的值
--map-column-hive <map> \ # 为已配置的列覆盖从SQL类型到配置单元类型的默认映射。如果在此参数中指定逗号，请使用URL编码的键和值
--check-column (col) \ # 指定在确定要导入哪些行时要检查的列
--incremental (mode) \ # 指定Sqoop如何确定哪些行是新行，mode的合法值包括append和lastmodified
--last-value (value) \ # 指定上次导入的校验列的最大值
```

> 注意：(4-6是Free-form Query，7-13是控制并行性 )
>
> 1.  --null-string和--null-non-string参数是可选的。如果未指定，则将使用字符串“null”。
> 2.  可以通过--columns参数控制导入列的顺序
> 3.  可以使用--where参数将WHERE子句追加到该语句中
> 4.  Sqoop可以通过--query参数指定SQL语句导入任意查询结果
> 5.  使用Free-form导入时，必须使用--target-dir指定目标目录
> 6.  如果您希望并行导入查询的结果，那么每个map任务都需要执行查询的副本，结果由Sqoop推断的边界条件进行分区。您的查询必须包括标记$CONDITIONS，每个Sqoop进程将用一个唯一的条件表达式替换它。您还必须使用--split-by选择拆分列
> 7.  默认情况下，使用四个map task。通过将-m或--num-mappers增加到8或16，一些数据库的性能可能会有所提高
> 8.  当执行并行导入时，Sqoop将识别表中的主键列(如果存在)并将其用作拆分列
> 9.  拆分列的低值和高值是从数据库中检索的，映射任务对总范围中大小相等的部分进行操作
> 10.  如果主键的实际值在其范围内不是均匀分布的，那么这会导致任务不平衡
> 11.  可以通过--split-by显示选择不同的列
> 12.  如果表没有定义主键，并且没有提供--split-by，则导入将失败，除非使用--num-mappers 1或者--autoreset-to-one-mapper是被使用的，将映射器的数量显式设置为1
> 13.  --autoreset-to-one-mapper通常与import-all-tables工具一起使用，以自动处理模式中没有主键的表
> 14.  --target-dir与--warehouse-dir不兼容
> 15.  使用直接模式时，可以指定应传递给基础工具的附加参数
> 16.  默认情况下，导入回转到新的目标位置，如果目标目录已经存在于HDFS,Sqoop将拒绝导入并覆盖该目录的内容
> 17.  如果你使用了--append参数，Sqoop会将数据导入到一个临时目录中，然后以不与该目录中现有文件名冲突的方式将文件重命名到正常的目标目录中
> 18.  增量导入：sqoop提供了一种增量导入模式，可用于仅检索比一些先前导入的行集更新的行，上述参数中，后三个参数为控制增量导入参数
> 19.  Sqoop支持两种类型的增量导入：append和lastmodified。可以使用--incremental参数指定类型
> 20.  当导入一个不断添加新行切行id值不断增加的表时，应该指定append模式，使用--check-column指定包含行id的列，Sqoop导入校验列的值大于用--last-value指定的值的行
> 21.  lastmodified:当源表的行可能被更新时，应该采用这种方法，每次更新都会将last-modified列的值设置为当前时间戳。如果check列保存的时间戳比用--last-value指定的时间戳更新，则导入该行
> 22.  在增量导入结束时，应该为后续导入指定为--last-value的值会打印到屏幕上，运行后续导入时，应该以这种方式指定--last-value，以确保只导入新的或更新的数据，这是通过将增量导入创建为保存的作业来自动处理的，这是执行重复增量导入的首选机制。
> 23.  --mysql-delimiters参数是一个简写参数，他使用mysqldump程序的默认分隔符。如果将mysqldump分隔符与直接模式导入(with --direct)结合使用，可以实现非常快速的导入
> 24.  虽然分隔符的选择对于文本模式导入是最重要的，但是如果使用--as-sequencefile导入到sequencefile，它仍是相关的，生成的类的toString()方法将使用指定的分隔符，因此输出数据集的后续格式化将依赖于选择的分隔符
> 25.  当Sqoop将数据导入到HDFS时，它会生成一个Java类，该类可以重新解释它在进行分隔格式导入时创建的文本文件，分隔符通过参数选择，如--fields-terminated-by;这控制了如何将数据写入磁盘，以及生成的parse()方法如何重新解释这些数据，通过使用--input-fields-terminated-by等，parse()方法使用的分隔符可以独立于输出参数进行选择
> 26.  SequenceFiles是一种二进制格式，它以自定义的特定于记录的数据类型存储各个记录，这些数据类型表现为Java类，Sqoop会自动为您生成这些数据类型，这种格式支持以二进制表示形式精确存储所有数据，并且适用于存储二进制数据（如，VARBINARY列），或者将由自定义MapReduce程序主要操作的数据（从SequenceFiles中读取比从文本文件中读取性能更高，因为不需要解析记录）
> 27.  Avro数据文件是一种紧凑、高效的二进制格式，提供了与用其他编程语言编写的应用程序的互操作性，Avro还支持版本控制，例如，当从表中添加或删除列时，以前导入的数据文件可以与新文件一起处理
> 28.  默认情况下，数据不会被压缩，可以使用deflate(gzip)算法和-z或--compress参数来压缩数据或者使用--compress-codec参数指定任何Hadoop压缩编解码器，适用于SequenceFile、text和Avro文件
> 29.  Sqoop的导入工具的主要功能是将数据上传到HDFS文件中，如果有一个与HDFS集群相关联的Hive metastore，则Sqoop还可以通过生成并执行CREATE TABLE语句来定义Hive中的数据布局，从而将数据导入Hive。将数据导入配置单元非常简单，只需在Sqoop命令行中添加--hive-import选项
> 30.  如果配置单元格已经存在，可以通过--hive-overwrite选项来指示必须替换配置单元中的现有表。在数据被导入到HDFS或省略这一步后，Sqoop将生成一个Hive脚本，其中包含使用Hive的类型定义列的CREATE TABLE操作，以及一个LOAD DATA INPATH语句，用于将数据文件移动到Hive的仓库目录中。

**导入的文件格式选择：分隔文本或序列化文件**

分割字符指令均为--fields-terminated-by X

而如下为支持的**转义字符**：

| 转义字符 | 描述   |
| -------- | ------ |
| \b       | 空格   |
| \n       | 换行   |
| \r       | 回车   |
| \t       | 制表符 |
| \\"      | 双引号 |
| \\\\'    | 单引号 |
| \\\      | 斜杠   |
| \0       | NUL    |

默认分隔符：

- 字段的逗号(,)
- 记录的换行(\n)
- 无引号字符
- 无转义字符

**import-all-tables tool将一组表从RDBMS导入到HDFS。每个表中的数据都存储在HDFS的一个单独的目录中，要使import-all-tables生效，则必须满足以下条件：**

- 每个表必须有一个单列主键，或者必须使用--autoreset-to-one-mapper选项
- 必须导入每个表的所有列
- 不能使用非默认拆分列，也不能通过WHERE子句强加任何条件

### 导入性能优化

默认情况下，Sqoop导入任务使用4个Map task, 可以通过-m或--num-mappers选项指定，可以考虑以下方式：

- 当数据量小于HDFS定义的块大小时，只需要使用一个Map task，可以有效减少MapReduce任务的执行时间，同时减少生成文件的数量，节省磁盘空间。
- 当数据量很大时，可以通过增加并行度来提高性能，但增加并行度并不总是最好的，通常情况下，并行度不应超过该结点上的MapReduce任务，可以从YARN请求的虚拟CPU的最大数量(对应的配置项是yarn.scheduler.maximum-allocation-vcores, 可以在yarn-site.xml配置，默认值为4)

可以使用--fetch-size来指定在执行导入时一次从数据库中读取的最大数据量，默认值为1000。建议从以下几个方面进行考虑：

- 要导入的表格是否是宽表格，是否包含大对象字段或长文本字段
- 数据库性能

如果数据库支持，使用--direct mode导入有时可以提高性能。此外，如果数据库支持，使用--relaxed-isolation选项指示Sqoop使用read uncommitted隔离级别导入数据可以提高数据传输速度







---

## Chapter 4

Sqoop导出工具可以将一组文件从HDFS导回RDBMS，目标表必须已经存在在数据库中，根据用户指定的分隔符，输入文件被读取并解析为一组记录。默认操作是将这些转换成一组将记录插入数据库的INSERT语句。

在update mode下，Sqoop将生成更新语句来替换数据库中的现有记录，而在call mode，Sqoop将为每个记录调用一个存储过程。

### **导出通用语句：**

sqoop export (generic-args) (export-args)

```shell
sqoop export --connect <jdbc-uri> --username <username> --password <password> \
--columns <col, col, col...> \ # 要导出到表中的列
--direct \ # 用直接导出快速路径
--export-dir <dir> \ # 导出HDFS源路径
-m.--num-mappers <n> \ # 在运行期间使用n个map tasks导出
--table <table-name> \ # 要填充的表
--call <stored-proc-name> \ # 要调用的存储过程
--update-mode <mode> \ # 指定在数据库中发现具有不匹配关键字的新行时如何执行更新
--update-key <col-name> \ # 用于更新的定位栏。如果有多列，请使用逗号分隔的列列表,mode的取值包括updateonly(默认值)和allowinsert
--input-null-string <null-string> \ # 对于字符串列，将替换为null
--input-null-non-string <null-string> \ # 对于非字符串列，将替换为null
--staging-table <staging-table-name> \ # 在将数据插入目标表之前，将在其中存放数据的表
--clear-staging-table \ # 表示可以删除临时表中的任何数据
--batch \ # 使用批处理模式执行底层语句
--validate \ # 启用复制数据的验证，仅支持单表复制
--validator <calss-name> \ # 指定要使用的验证程序类
--validation-threshold <class-name> \ # 指定要使用的验证阈值类
--validation-failurehandler <class-name> \ # 指定要使用的验证失败处理程序类
```

> 注意：
>
> 1. 当使用export 工具时，必须使用--export-dir参数指定HDFS中包含源数据的目录，并指定要通过--table导出到数据库的目标表，或者指定要通过--call调用的存储过程。**--table和--call不能同时使用**
> 2. 默认情况下，选择表中的所有列进行导出。还可以使用--columns参数来指示要导出哪些列，并控制导出列的顺序，其值堆应于以逗号分隔的列名列表
> 3. 一些数据库还提供了direct导出模式.使用--direct参数指定此代码路径。这可能比标准的JDBC码路径具有更高的性能
> 4. 由于Sqoop将导出过程分解为多个事务，因此失败的导出作业可能回导致部分数据被提交到数据库。在某些情况下，这会进一步导致后续作业由于插入冲突而失败，或者在其他情况下导致重复数据。可以通过--staging-table选项指定一个临时表来解决这个问题，该临时表充当用于临时存储导出数据的辅助表，在单个事务中，暂存的数据最终被移动到目标表中
> 5. 为了使用分段工具，必须在运行导出作业之前创建分段表，该表在结构上必须与目标表相同。该表在导出作业运行之前应该为空，或者必须指定--clear-staging-table选项。如果临时表包含数据并且指定了--clear-staging-table选项，则Sqoop将在开始导出作业之前删除所有数据
> 6. 需要--export-dir参数和--table或--call中之一。他们指定要填充到数据库中的表(或要调用的存储过程)，以及包含元数据的HDFS目录
> 7. Sqoop export操作将输入记录转换为Insert语句，一条语句最多可插入100条记录
> 8. 负责写入数据库的任务将为它们接收的每1000条记录提交一个事务，这保证了事务缓冲区不会溢出，并避免了内存消耗的风险
> 9. 要验证导出的结果，可以使用--validation,验证的目的是比较导出前后记录的差异，从而知道导出的结果是否符合预期



**插入模式与更新模式(Insert mode&Update mode)**

有两种导出数据的模式，即Insert和Update。Insert mode常用于完全导出，即将数据导出到一个空表中。Update mode有时用于增量导出，Update mode有两种，默认为updateonly，也可指定为allowinsert

- Insert mode: 如果没有指定--update-key，Sqoop将使用默认的Insert mode完成导出，它将把每条记录转换成对数据库表的Insert 语句，如果目标表有一些约束，如唯一约束，使用Insert mode要小心，避免违反那些约束。如果一条记录插入失败，整个导出作业将最终失败。这种插入模式通常用于将数据导出到新的空表中。

- Update mode: 如果指定了--update-key, 将使用Update  mode完成导出，默认的模式是updateonly，或者可以添加--update-mode updateonly来显示设置。在updateonly模式下，Sqoop将只修改数据库表中已存在的数据集，作为输入的每条记录将被转换为update语句来修改现有记录，要修改的记录由--update-key指定的列决定。如果UPDATE语句在数据库中没有相应的记录，它不会插入新数据，但也不会报错，导出操作将继续。简而言之，新记录不会导出到数据库中，而只会更新现有的记录。还可以通过添加--update-mode allowinsert将更新模式指定为allowinsert, 然后可以更新现有记录并同时插入新纪录。对每条记录执行更新操作还是插入操作由--update-key指定的列决定，且只有当所有引用列都匹配时，才会进行更新

  

在Call mode下，Sqoop为每条记录调用一个存储过程来插入或更新数据，需要事先在数据库上创建存储过程：

```shell
sqoop-export --connect jdbc:mysql://db.example.com/foo --call barproc \
 --export-dir /results/bar_data
```



**导出失败处理（以及可能的原因）：**

- 从Hadoop集群到数据库的连接丢失(由于硬件故障或服务器软件崩溃)
- 试图插入违反一致性约束的行（例如，插入重复的主键值）
- 试图从HDFS源数据中解析不完整或格式不正确的记录
- 试图使用不正确的分隔符分析记录
- 容量问题（内存或磁盘空间不足）



**从Hive导出数据：**

Sqoop不支持直接从配置单元表中导出数据，您只能用--export-dir选项从HDFS中导出数据，步骤如下：

- 确定要导出的Hive表的结构，是否是分区表，是否启用压缩等
- 确定Hive表中数据在HDFS的实际存储位置
- 确定源数据的分隔符设置
- 基于用于导出数据的Hive表在数据库中创建具有相同结构的表
- 使用Sqoop export工具编写一个命令，将数据导出到数据库的目标表中，注意：
  - 使用--export-dir正确指定HDFS中的配置单元数据所在的目录
  - 分隔符设置应该与源表匹配



**从HBase导出数据：**

Sqoop不支持直接从HBase表中导出数据，但可以在Hive表的帮助下间接完成：

- 基于配置单元中的HBase表创建外部表
- 基于刚刚创建的的配置单元外部表创建配置单元内部表
- 将数据从配置单元外部表加载到配置单元内部表中
- 将数据从配置单元内部表导出到我们预先在数据库中创建的目标表中
- 如有必要，清理配置单元临时表



**导出验证的三个基本接口：**

- ValidationThreshold: 确定源和目标直接的误差是否可接受：绝对误差、百分比公差等，默认实现是AbsoluteValidationThreshold, 它确保来自源和目标的函数相同
- ValidationFailureHandler: 负责处理失败：记录错误/警告、中止等。默认实现是LogOnFailureHandler，它将警告消息记录到配置的记录器
- Validator: 通过将决策委托给ValidationThreshold并将失败处理委托给ValidationFailureHandler来驱动验证逻辑。默认实现是RowCountValidator，它验证来自源和目标的行计数

这些接口可以在org.apache.sqoop.validation包下找到，下面为一个将HDFS数据导出到数据库中的条形表并启用行数校验的简单示例：

```shell
sqoop export --connect jdbc:mysql://db.example.com/foo --table bar \
--export-dir /results/bar_data –validate
```



### 导出性能优化

- 当目标数据库支持时，在命令中使用--direct参数可以提高性能
- 默认情况下，Sqoop的导出函数对导出的每一行数据执行一条INSERT语句。如果数据量大的时候想提高导出速度，可以设置单条INSERT语句批量插入多行数据：
  - 在命令中添加--batch选项以启用JDBC批处理
  - 修改每条SQL语句可批量导出的记录行数
  - 设置单个事务提交的查询语句数
- 设置-Dsqoop.export.statements.per.transaction=10000, 我们可以指定在单个事务中将执行多少条INSERT语句。较高的值通常有助于提高性能，但取决于数据库
- 通过在Sqoop导出命令中添加-Djdbc.transaction.isolation=TRANSCATION_READ_UNCOMMITTED,可以将数据库的事务隔离级别修改为read uncommitted，提高导出速度，以降低事务为代价避免死锁等问题的隔离级别。







---

## Chapter 5

### Sqoop Job

一个已经保存的job保存了执行指定Sqoop命令的所有信息，已保存的job一旦创建，可以随时执行。默认情况下，job描述保存在$SQOOP_HOME/.sqoop/中的私有存储库中。您可以将Sqoop配置为使用共享原存储，这使得保存的作业可供共享集群中的多个用户使用。

**通用基础指令：**

```shell
sqoop job (generic-args) (job-args) [-- [subtool-name] (subtool-args)]
```

Job管理参数：

| 参数               | 描述                                                         |
| ------------------ | ------------------------------------------------------------ |
| --create \<job-id> | 使用指定的job-id（名称）定义新保存的job。应该指定第二个Sqoop命令行，由--分隔；这定义了保存的job |
| --delete \<job-id> | 删除一个已保存的job                                          |
| --exec \<job-id>   | 给定一个用--create定义的作业，运行保存的作业                 |
| --show \<job-id>   | 展示这个job的参数                                            |
| --list             | 列出所有job                                                  |

创建已保存的job是通过--create操作完成的，如：

```shell
sqoop job --create myjob -- import --connect jdbc:mysql://example.com/db \
--table mytable
```

如果我们对这个job满意，我们可以通过--exec来运行这个job，如：

```shell
sqoop job --exec myjob
```



**元数据连接选项：**

| 参数                       | 描述                                    |
| -------------------------- | --------------------------------------- |
| --meta-connect \<jdbc-uri> | 指定用于连接到Metastore的JDBC连接字符串 |

Metastore仓库可以在conf/sqoop-site.xml文件中配置，元数据默认存储在HSQLDB内存级数据库中，metastore文件在磁盘上的位置由sqoop.metastore.server.location属性控制。这应该指向本地文件系统上的一个目录

Metastore通过TCP/IP可用，默认端口为16000，可以通过sqoop.metastore.server.port属性控制

可以通过--meta-connect属性指定如何连接到metastore服务，如：

```shell
--meta-connect jdbc:hsqldb:hsql:// metaserver.example.com:16000/sqoop
```

如果不指定--meta-connect选项就想自动连接到metastore仓库，可以使用sqoop-site.xml中的sqoop.metastore.client.enable.autoconnect属性,并赋值为True，以及将sqoop.metastore.client.autoconnect.url属性设置为正确的url地址，如果未指定，将使用私有元数据存储库，默认情况下Metastore的文件位于$SQOOP_HOME/.sqoop/

可以使用sqoop-metastore工具启动和关闭Sqoop Metastore Service:

- 启动：`sqoop metastore`
- 关闭：`sqoop metastore -shutdown`



### Sqoop Merge Tool

Merge工具允许合并两个数据集，其中一个数据集的条目应该覆盖旧数据集中的条目。

**通用基础指令：**

```shell
sqoop merge (generic-args) (merge-args)
```

**Merge选项参数：**

| 参数                  | 描述                                       |
| --------------------- | ------------------------------------------ |
| --class-name \<class> | 指定要在合并作业期间使用的记录特定类的名称 |
| --jar-file \<file>    | 指定要从中加载记录类的jar的名称            |
| --merge-key \<col>    | 指定要用作merge key的列的名称              |
| --new-data \<path>    | 指定较新数据集的路径                       |
| --onto \<path>        | 指定老的数据集的路径                       |
| --target-dir \<path>  | 指定合并作业输出的目标路径                 |

Merge tool运行一个MapReduce作业，该作业将目录作为输入：一个较新的数据集和一个较老的数据集，输出将放置在--target-dir指定的HDFS中的目录。合并数据集时，假定每条记录中都有一个唯一的主键值，则主键的列用--merge-key指定。同一个数据集中的多行不能有相同的主键，否则可能会丢失数据。

Merge tool通常在使用lastmodified mode(sqoop import --incremental lastmodified)进行增量导入后运行，以将新导入的数据合并到旧数据上。

### Sqoop Code Generation Tool

Sqoop Code Generation Tool来解析和解释包含要导出回数据库的数据的文件记录，如果这些文件是使用非默认分隔符创建的（逗号分隔字段和换行符分隔记录），您应该再次指定相同的分隔符，以便Sqoop可以解析您的文件。

Code Generation 参数对于sqoop-import和sqoop-export工具：

| 参数                   | 描述                                                         |
| ---------------------- | ------------------------------------------------------------ |
| --bindir \<dir>        | 编译对象的输出目录                                           |
| --class-name \<name>   | 设置生成的类名。这会覆盖--package-name与--jar-file结合使用时，设置输入类 |
| --jar-file \<file>     | 禁用代码生成，使用指定jar包                                  |
| --outdir \<dir>        | 生成代码的输出路径                                           |
| --package-name \<name> | 将自动生成的类放在这个包中                                   |
| --map-column-java \<m> | 覆盖已配置列的从SQL类型到Java类型的默认映射                  |

生成的类名一般与表名相同，也可以使用--class-name选项指定。同样，您可以使用--package-name选项仅指定包名称：

```shell
sqoop import --connect <connect-str> --table SomeTable --package-name com.foocorp
```

类的.java源文件将写入运行sqoop的当前工作目录，可以使用--outdir控制输出目录，如：`--outdir src/generated`

导入过程将源代码编译成.class和.jar文件；这些通常存储在/tmp下，您可以使用--bindir选择备用目标目录。如果已经有一个可用于执行导入的已编译类，并且想要抑制导入过程的代码生成方面，可以通过--jar-file和--class-name来使用已存在的jar和类

请注意--update-key选项不能与--jar-file和--class-name选项一起使用，因为更新模式导出必须使用新生成的代码进行解析。如果生成的代码丢失，可以使用Sqoop Code Generation Tool重新生成

如果向Code Generation Tool提供Hive参数，Sqoop会生成一个包含HQL语句的文件，以创建表和加载数据,如：

```shell
sqoop codegen --connect jdbc:mysql://db.foo.com/corp \
--table employees
```

### Sqoop Create Hive Table Tool

create-hive-table工具使用基于数据库表的表定义填充Hive原存储，这有效地执行了sqoop-import的"--hive-import"步骤，而无需运行前面的导入

**通用语法：**

```shell
sqoop create-hive-table (generic-args) (create-hive-table-args)
```

请注意，新的Hive表将在名为default的Hive默认数据库下创建。可以通过--hive-table指定表，或者使用--hive-database单独指定导入Hive的数据库的名称

### Sqoop Evaluation Tool

eval tool允许用户对数据库快速运行简单的SQL查询，结果打印到控制台

**通用语法：**

```shell
sqoop eval (generic-args) (eval-args)
```

